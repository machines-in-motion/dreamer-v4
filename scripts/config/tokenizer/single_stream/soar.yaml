# @package _global_
defaults:
 - /dataset: soar

reload_checkpoint: null
output_dir: checkpoints/tokenizer/soar/${now:%Y-%m-%d_%H-%M-%S}
plot_every: 1000
print_every: 100
log_every: 25
save_every: 5000
seed: 42

tokenizer:
  dual_stream: false
  num_modality_tokens: 256    # Number of tokens for the image or other modalities
  num_latent_tokens: 256      # The number of latent tokens per frame
  max_context_length: 32      # The maximum temporal horizon of the tokenizer
  model_dim: 1024              # The dimension of the transformer latents
  latent_dim: 32              # The dimention of the encoder output latents (z)
  enc_num_layers: 4           # Causal tokenizer encoder depth
  dec_num_layers: 4           # Causal tokenizer decoder depth
  n_heads: 16
  n_kv_heads: 16
  dropout_prob: 0.0
  qk_norm: false
  patch_size: 16 # Note: image_size//patch_size should be equal to num_modality_tokens

train:
  use_compile: false
  num_epochs: 60
  batch_per_gpu: 1
  grad_accum_steps: 1
  lr: 1e-4
  num_workers: 12
  lpips_weight: 0.2
  num_training_steps: 1e7
  mixed_precision: false
  torch_compile_mode: "default"
  enable_flash_attention: false
  enable_fast_matmul: true
  clip_grad_norm: 1.0

wandb:
  enable: false 
  api_key: null
  project: "dreamer-v4-tokenizer"
  run_id: null




# Model hyperparameters
    # IMG_H, IMG_W = 256, 256
    # CONTEXT_T = 96
    # N_LATENTS = 512
    # BOTTLENECK_D = 16
    # D_MODEL = 1024
    # N_LAYERS = 16
    # HEADS_Q = 16
    # HEADS_KV_LATENT = 16
    # MLP_RATIO = 4.0
    # TEMPORAL_EVERY = 4
    
    # # Batch size per GPU
    # BATCH_PER_GPU = 1
    # T = CONTEXT_T
    # NUM_EPOCHS = 65
    # # STEPS_PER_EPOCH = 500  # Limit steps per epoch for faster benchmarking
    # GRAD_ACCUM_STEPS = 4  # simulate batch size 10 per GPU